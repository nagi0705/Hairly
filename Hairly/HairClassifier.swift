import CoreML
import Vision
import UIKit

class HairClassifier {
    static let shared = HairClassifier()
    
    private var model: Hairly_ML_1?
    
    init() {
        do {
            let config = MLModelConfiguration()
            model = try Hairly_ML_1(configuration: config)
            print("‚úÖ CoreML„É¢„Éá„É´„Çí„É≠„Éº„Éâ„Åó„Åæ„Åó„Åü: \(String(describing: model))")
        } catch {
            print("‚ùå CoreML„É¢„Éá„É´„ÅÆ„É≠„Éº„Éâ„Å´Â§±Êïó: \(error.localizedDescription)")
            model = nil // Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÄÅnil„ÇíË®≠ÂÆö
        }
    }
    
    // üìå ÁîªÂÉè„ÇíÂàÜÈ°û„Åô„Çã„É°„ÇΩ„ÉÉ„ÉâÔºàÁ¢∫ÁéáÂàÜÂ∏É„ÅÆ„É≠„Ç∞Âá∫Âäõ„ÇíÊîπËâØÔºâ
    func classify(image: UIImage, completion: @escaping (String?, HairStyle?) -> Void) {
        guard let model = model else {
            print("‚ùå „É¢„Éá„É´„Åå„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì")
            completion(nil, nil)
            return
        }
        
        guard let pixelBuffer = image.toCVPixelBuffer() else {
            print("‚ùå UIImage „Åã„Çâ PixelBuffer „Å∏„ÅÆÂ§âÊèõ„Å´Â§±Êïó")
            completion(nil, nil)
            return
        }
        
        do {
            let prediction = try model.prediction(image: pixelBuffer)
            let result = prediction.target // üî• Ë™çË≠ò„Åï„Çå„ÅüÈ´™Âûã„ÅÆ„É©„Éô„É´
            let probabilities = prediction.targetProbability // üî• È´™Âûã„Åî„Å®„ÅÆÁ¢∫ÁéáÂàÜÂ∏É
            
            // üìä **Á¢∫ÁéáÂàÜÂ∏É„ÇíË¶ã„ÇÑ„Åô„ÅèË°®Á§∫**
            print("‚úÖ Ë™çË≠òÁµêÊûú: \(result)")
            print("üìä È´™Âûã„ÅÆÁ¢∫ÁéáÂàÜÂ∏É:")
            for (key, value) in probabilities {
                print("   üîπ \(key): \(String(format: "%.2f%%", value * 100))") // ‰æã: 85.45%
            }

            // È´™Âûã„ÅÆË™¨Êòé„Éá„Éº„Çø„ÇíÂèñÂæó
            let hairStyleInfo = HairStyleManager.shared.getHairStyle(for: result) // üî• ‰øÆÊ≠£: getHairStyle(for:) „Çí‰ΩøÁî®
            
            completion(result, hairStyleInfo)
        } catch {
            print("‚ùå ÁîªÂÉèËß£Êûê„Ç®„É©„Éº: \(error.localizedDescription)")
            completion(nil, nil)
        }
    }
}

// üìå UIImage ‚Üí CVPixelBuffer Â§âÊèõÁî®„ÅÆÊã°Âºµ
extension UIImage {
    func toCVPixelBuffer() -> CVPixelBuffer? {
        let width = 299
        let height = 299
        let attributes: [CFString: Any] = [
            kCVPixelBufferCGImageCompatibilityKey: true,
            kCVPixelBufferCGBitmapContextCompatibilityKey: true
        ]
        var pixelBuffer: CVPixelBuffer?
        let status = CVPixelBufferCreate(kCFAllocatorDefault, width, height,
                                         kCVPixelFormatType_32BGRA,
                                         attributes as CFDictionary,
                                         &pixelBuffer)
        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            print("‚ùå PixelBuffer„ÅÆ‰ΩúÊàê„Å´Â§±Êïó: \(status)")
            return nil
        }

        CVPixelBufferLockBaseAddress(buffer, .readOnly)
        guard let context = CGContext(data: CVPixelBufferGetBaseAddress(buffer),
                                      width: width, height: height,
                                      bitsPerComponent: 8, bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
                                      space: CGColorSpaceCreateDeviceRGB(),
                                      bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue) else {
            print("‚ùå CGContext„ÅÆ‰ΩúÊàê„Å´Â§±Êïó")
            CVPixelBufferUnlockBaseAddress(buffer, .readOnly)
            return nil
        }
        
        guard let cgImage = self.cgImage else {
            print("‚ùå UIImage „Åã„Çâ CGImage „Å∏„ÅÆÂ§âÊèõ„Å´Â§±Êïó")
            CVPixelBufferUnlockBaseAddress(buffer, .readOnly)
            return nil
        }

        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        CVPixelBufferUnlockBaseAddress(buffer, .readOnly)

        return buffer
    }
}
